{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to JAX\n",
    "\n",
    "This notebook provides a brief introduction to [JAX](https://jax.readthedocs.io/), a library for high-performance numerical computing with automatic differentiation. JAX is particularly useful for scientific computing and machine learning because it allows us to:\n",
    "\n",
    "1. Write NumPy-like code that runs fast\n",
    "2. Automatically compute derivatives of functions\n",
    "\n",
    "Other benefits (but not necessarily used in the project):\n",
    "\n",
    "3. Easily vectorize computations\n",
    "4. Compile code for execution on GPUs\n",
    "\n",
    "These features make JAX a great choice for implementing Physics-Informed Neural Networks (PINNs), and is what is often used by researchers in this field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JAX vs NumPy: Familiar Syntax\n",
    "\n",
    "If you know NumPy, you already know most of JAX! The `jax.numpy` module provides almost the same API as NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy array: [1. 2. 3.]\n",
      "Sum: 6.0\n",
      "Mean: 2.0\n"
     ]
    }
   ],
   "source": [
    "# NumPy\n",
    "x_np = np.array([1.0, 2.0, 3.0])\n",
    "print(\"NumPy array:\", x_np)\n",
    "print(\"Sum:\", np.sum(x_np))\n",
    "print(\"Mean:\", np.mean(x_np))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX array: [1. 2. 3.]\n",
      "Sum: 6.0\n",
      "Mean: 2.0\n"
     ]
    }
   ],
   "source": [
    "# JAX is almost identical, only replace np with jnp!\n",
    "x_jax = jnp.array([1.0, 2.0, 3.0])\n",
    "print(\"JAX array:\", x_jax)\n",
    "print(\"Sum:\", jnp.sum(x_jax))\n",
    "print(\"Mean:\", jnp.mean(x_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most NumPy operations work the same way in JAX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A:\n",
      " [[1. 2.]\n",
      " [3. 4.]]\n",
      "Vector b: [1. 2.]\n",
      "A @ b: [ 5. 11.]\n",
      "A.T:\n",
      " [[1. 3.]\n",
      " [2. 4.]]\n",
      "jnp.linalg.inv(A):\n",
      " [[-2.0000002   1.0000001 ]\n",
      " [ 1.5000001  -0.50000006]]\n"
     ]
    }
   ],
   "source": [
    "# Matrix operations\n",
    "A = jnp.array([[1.0, 2.0], [3.0, 4.0]])\n",
    "b = jnp.array([1.0, 2.0])\n",
    "\n",
    "print(\"Matrix A:\\n\", A)\n",
    "print(\"Vector b:\", b)\n",
    "print(\"A @ b:\", A @ b)\n",
    "print(\"A.T:\\n\", A.T)\n",
    "print(\"jnp.linalg.inv(A):\\n\", jnp.linalg.inv(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [0.        2.0943952 4.1887903 6.2831855]\n",
      "sin(x): [ 0.0000000e+00  8.6602539e-01 -8.6602545e-01  1.7484555e-07]\n",
      "exp(x): [  1.         8.120528  65.94297  535.49176 ]\n"
     ]
    }
   ],
   "source": [
    "# Element-wise operations\n",
    "x = jnp.linspace(0, 2 * jnp.pi, 4)\n",
    "print(\"x:\", x)\n",
    "print(\"sin(x):\", jnp.sin(x))\n",
    "print(\"exp(x):\", jnp.exp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert between NumPy and JAX arrays using `jnp.asarray()` and `np.asarray()`. However, try to minimize conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to NumPy: [1. 2. 3.]\n",
      "Type: <class 'numpy.ndarray'>\n",
      "\n",
      "Converted to JAX: [4. 5. 6.]\n",
      "Type: <class 'jaxlib._jax.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "jax_array = jnp.array([1.0, 2.0, 3.0])\n",
    "numpy_array = np.array([4.0, 5.0, 6.0])\n",
    "\n",
    "# Convert JAX array to NumPy array\n",
    "converted_to_numpy = np.asarray(jax_array)\n",
    "print(\"Converted to NumPy:\", converted_to_numpy)\n",
    "print(\"Type:\", type(converted_to_numpy))\n",
    "\n",
    "# Convert NumPy array to JAX array\n",
    "converted_to_jax = jnp.asarray(numpy_array)\n",
    "print(\"\\nConverted to JAX:\", converted_to_jax)\n",
    "print(\"Type:\", type(converted_to_jax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Array Immutability\n",
    "\n",
    "One key difference from NumPy: **JAX arrays are immutable**. You cannot modify them in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy after modification: [10  2  3]\n"
     ]
    }
   ],
   "source": [
    "# This works in NumPy\n",
    "x_np = np.array([1, 2, 3])\n",
    "x_np[0] = 10\n",
    "print(\"NumPy after modification:\", x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This does NOT work in JAX - uncomment to see the error\n",
    "x_jax = jnp.array([1, 2, 3])\n",
    "# x_jax[0] = 10  # TypeError: JAX arrays are immutable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, JAX provides the `.at` property for creating modified copies of arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original x: [1 2 3 4 5]\n",
      "New array:  [10  2  3  4  5]\n",
      "Set slice:  [ 1 99 99  4  5]\n"
     ]
    }
   ],
   "source": [
    "x = jnp.array([1, 2, 3, 4, 5])\n",
    "\n",
    "# Set a single element\n",
    "x_new = x.at[0].set(10)\n",
    "print(\"Original x:\", x)  # unchanged!\n",
    "print(\"New array: \", x_new)\n",
    "\n",
    "# Set multiple elements\n",
    "x_new2 = x.at[1:3].set(99)\n",
    "print(\"Set slice: \", x_new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 2.]\n",
      " [0. 5. 2.]\n",
      " [0. 0. 2.]]\n"
     ]
    }
   ],
   "source": [
    "# 2D array example\n",
    "A = jnp.zeros((3, 3))\n",
    "A = A.at[0, :].set(1.0)  # Set first row to 1\n",
    "A = A.at[:, 2].set(2.0)  # Set last column to 2\n",
    "A = A.at[1, 1].set(5.0)  # Set center element to 5\n",
    "print(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic Differentiation with `jax.grad`\n",
    "\n",
    "This is main reason why we use JAX in this project! The `jax.grad` function automatically computes the gradient/derivative of a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(3.0) = 9.0\n",
      "f'(3.0) = 6.0\n"
     ]
    }
   ],
   "source": [
    "# Define a simple function f(x) = x^2\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "\n",
    "# Evaluate at x = 3.0\n",
    "x = 3.0\n",
    "\n",
    "# Create the gradient function: f'(x) = 2x, and evaluate it at x = 3.0\n",
    "f_x = jax.grad(f)(x)\n",
    "\n",
    "print(f\"f({x}) = {f(x)}\")\n",
    "print(f\"f'({x}) = {f_x}\")  # Should be 2 * 3 = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g(1.0) = 0.309560\n",
      "g'(1.0) = -0.110794\n",
      "Analytical: -0.110794\n"
     ]
    }
   ],
   "source": [
    "# A more complex function: f(x) = sin(x) * exp(-x)\n",
    "def g(x):\n",
    "    return jnp.sin(x) * jnp.exp(-x)\n",
    "\n",
    "\n",
    "# Evaluate at x = 1.0\n",
    "x = 1.0\n",
    "\n",
    "# The derivative: g'(x) = cos(x)*exp(-x) - sin(x)*exp(-x) = exp(-x)*(cos(x) - sin(x))\n",
    "g_x = jax.grad(g)(x)\n",
    "\n",
    "print(f\"g({x}) = {g(x):.6f}\")\n",
    "print(f\"g'({x}) = {g_x:.6f}\")\n",
    "\n",
    "# Verify analytically\n",
    "analytical = jnp.exp(-x) * (jnp.cos(x) - jnp.sin(x))\n",
    "print(f\"Analytical: {analytical:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop and think about how amazing this is! You can define any function using standard JAX operations, and `jax.grad` will give you a new function that computes its derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second-Order Derivatives\n",
    "\n",
    "To compute second derivatives, simply apply `grad` twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2.0) = 8.0\n",
      "f'(2.0) = 12.0\n",
      "f''(2.0) = 12.0\n"
     ]
    }
   ],
   "source": [
    "# f(x) = x^3\n",
    "# f'(x) = 3x^2\n",
    "# f''(x) = 6x\n",
    "def f(x):\n",
    "    return x**3\n",
    "\n",
    "\n",
    "x = 2.0\n",
    "\n",
    "f_x = jax.grad(f)(x)  # First derivative\n",
    "f_xx = jax.grad(jax.grad(f))(x)  # Second derivative\n",
    "\n",
    "print(f\"f({x}) = {f(x)}\")\n",
    "print(f\"f'({x}) = {f_x}\")  # Should be 3 * 4 = 12\n",
    "print(f\"f''({x}) = {f_xx}\")  # Should be 6 * 2 = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradients of Multivariate Functions\n",
    "\n",
    "For functions of multiple variables, `grad` computes partial derivatives. By default, it differentiates with respect to the first argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(2.0, 3.0) = 39.0\n",
      "df/dx = 12.0\n",
      "df/dy = 31.0\n"
     ]
    }
   ],
   "source": [
    "# f(x, y) = x^2 * y + y^3\n",
    "# df/dx = 2xy\n",
    "# df/dy = x^2 + 3y^2\n",
    "def f(x, y):\n",
    "    return x**2 * y + y**3\n",
    "\n",
    "\n",
    "x, y = 2.0, 3.0\n",
    "\n",
    "# Gradient with respect to x (first argument, argnums=0 is default)\n",
    "f_x = jax.grad(f, argnums=0)(x, y)\n",
    "\n",
    "# Gradient with respect to y (second argument)\n",
    "f_y = jax.grad(f, argnums=1)(x, y)\n",
    "\n",
    "print(f\"f({x}, {y}) = {f(x, y)}\")\n",
    "print(f\"df/dx = {f_x}\")  # 2 * 2 * 3 = 12\n",
    "print(f\"df/dy = {f_y}\")  # 4 + 27 = 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second-Order Partial Derivatives\n",
    "\n",
    "For PDEs, we often need second derivatives like $\\frac{\\partial^2 f}{\\partial x^2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f(x, y) = 0.353553\n",
      "d2f/dx2 = -0.353553\n",
      "d2f/dy2 = -0.353553\n",
      "d2f/dxdy = -0.612372\n",
      "\n",
      "Analytical d2f/dx2 = -0.353553\n"
     ]
    }
   ],
   "source": [
    "# f(x, y) = sin(x) * cos(y)\n",
    "# d2f/dx2 = -sin(x) * cos(y)\n",
    "# d2f/dy2 = -sin(x) * cos(y)\n",
    "# d2f/dxdy = -cos(x) * sin(y)\n",
    "def f(x, y):\n",
    "    return jnp.sin(x) * jnp.cos(y)\n",
    "\n",
    "\n",
    "x, y = jnp.pi / 4, jnp.pi / 3\n",
    "\n",
    "# Second derivative with respect to x\n",
    "f_xx = jax.grad(jax.grad(f, argnums=0), argnums=0)(x, y)\n",
    "\n",
    "# Second derivative with respect to y\n",
    "f_yy = jax.grad(jax.grad(f, argnums=1), argnums=1)(x, y)\n",
    "\n",
    "# Mixed derivative\n",
    "f_xy = jax.grad(jax.grad(f, argnums=0), argnums=1)(x, y)\n",
    "\n",
    "print(f\"f(x, y) = {f(x, y):.6f}\")\n",
    "print(f\"d2f/dx2 = {f_xx:.6f}\")\n",
    "print(f\"d2f/dy2 = {f_yy:.6f}\")\n",
    "print(f\"d2f/dxdy = {f_xy:.6f}\")\n",
    "\n",
    "# Verify: d2f/dx2 = -sin(x)*cos(y)\n",
    "print(f\"\\nAnalytical d2f/dx2 = {-jnp.sin(x) * jnp.cos(y):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `jax.value_and_grad`: Get Both Value and Gradient\n",
    "\n",
    "Often we need both the function value and its gradient (e.g., for optimization). Computing them separately would be wasteful. `jax.value_and_grad` computes both efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [1. 2. 3.]\n",
      "loss(x) = 14.0\n",
      "grad(loss)(x) = [2. 4. 6.]\n"
     ]
    }
   ],
   "source": [
    "def loss(x):\n",
    "    return jnp.sum(x**2)\n",
    "\n",
    "\n",
    "x = jnp.array([1.0, 2.0, 3.0])\n",
    "\n",
    "# Create a function that returns (value, gradient), and evaluate it\n",
    "value, gradient = jax.value_and_grad(loss)(x)\n",
    "\n",
    "print(f\"x = {x}\")\n",
    "print(f\"loss(x) = {value}\")\n",
    "print(f\"grad(loss)(x) = {gradient}\")  # Should be 2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is particularly useful in training loops where we need the loss value for monitoring and the gradient for updating parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT Compilation with `jax.jit`\n",
    "\n",
    "`jax.jit` compiles functions using XLA (Accelerated Linear Algebra) for faster execution. The first call is slower (compilation), but subsequent calls are much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_function(x):\n",
    "    \"\"\"A function with many operations.\"\"\"\n",
    "    for _ in range(100):\n",
    "        x = x @ x.T\n",
    "        x = x / jnp.linalg.norm(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# JIT-compile the function\n",
    "fast_function = jax.jit(slow_function)\n",
    "\n",
    "x = jnp.array([[1.0, 2.0], [3.0, 4.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.58 ms ± 72.4 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# Time the non-jitted version\n",
    "%timeit slow_function(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.3 μs ± 2.12 μs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "# First call includes compilation time\n",
    "_ = fast_function(x)\n",
    "\n",
    "# Time the jitted version (after compilation)\n",
    "%timeit fast_function(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a shortcut we usually define the functions we want to JIT using the `@jax.jit` decorator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def fast_function(x):\n",
    "    \"\"\"A function with many operations.\"\"\"\n",
    "    for _ in range(100):\n",
    "        x = x @ x.T\n",
    "        x = x / jnp.linalg.norm(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining `jit` with `grad`\n",
    "\n",
    "JAX transformations compose nicely. You can JIT-compile a gradient function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient: [2. 4. 6.]\n"
     ]
    }
   ],
   "source": [
    "def loss(params, x):\n",
    "    \"\"\"A simple loss function.\"\"\"\n",
    "    return jnp.sum((params - x) ** 2)\n",
    "\n",
    "\n",
    "# JIT-compiled gradient function\n",
    "grad_loss = jax.jit(jax.grad(loss))\n",
    "\n",
    "# Or equivalently\n",
    "grad_loss = jax.grad(loss)\n",
    "grad_loss = jax.jit(grad_loss)\n",
    "\n",
    "params = jnp.array([1.0, 2.0, 3.0])\n",
    "x = jnp.array([0.0, 0.0, 0.0])\n",
    "print(f\"Gradient: {grad_loss(params, x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization with `jax.vmap`\n",
    "\n",
    "`jax.vmap` (vectorizing map) automatically converts a function that operates on single examples to one that operates on batches. This is much faster than using Python loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of vectors:\n",
      "[[3. 4.]\n",
      " [1. 0.]\n",
      " [0. 5.]\n",
      " [1. 1.]]\n",
      "\n",
      "Normalized (using vmap):\n",
      "[[0.6        0.8       ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.70710677 0.70710677]]\n"
     ]
    }
   ],
   "source": [
    "# A function that operates on a single vector\n",
    "def normalize(x):\n",
    "    \"\"\"Normalize a single vector.\"\"\"\n",
    "    return x / jnp.linalg.norm(x)\n",
    "\n",
    "\n",
    "# Batch of vectors (each row is a vector)\n",
    "batch = jnp.array([[3.0, 4.0], [1.0, 0.0], [0.0, 5.0], [1.0, 1.0]])\n",
    "\n",
    "# Use vmap to apply normalize to each row\n",
    "batch_normalize = jax.vmap(normalize)\n",
    "result = batch_normalize(batch)\n",
    "\n",
    "print(\"Batch of vectors:\")\n",
    "print(batch)\n",
    "print(\"\\nNormalized (using vmap):\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch of vectors:\n",
      "[[3. 4.]\n",
      " [1. 0.]\n",
      " [0. 5.]\n",
      " [1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Batch of vectors (each row is a vector)\n",
    "batch = jnp.array([[3.0, 4.0], [1.0, 0.0], [0.0, 5.0], [1.0, 1.0]])\n",
    "\n",
    "print(\"Batch of vectors:\")\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using loop:\n",
      "[[0.6        0.8       ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.70710677 0.70710677]]\n"
     ]
    }
   ],
   "source": [
    "# The slow way: Python loop\n",
    "result_loop = jnp.array([normalize(batch[i]) for i in range(len(batch))])\n",
    "print(\"Using loop:\")\n",
    "print(result_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using vmap:\n",
      "[[0.6        0.8       ]\n",
      " [1.         0.        ]\n",
      " [0.         1.        ]\n",
      " [0.70710677 0.70710677]]\n"
     ]
    }
   ],
   "source": [
    "# The fast way: vmap\n",
    "batch_normalize = jax.vmap(normalize)\n",
    "result_vmap = batch_normalize(batch)\n",
    "print(\"Using vmap:\")\n",
    "print(result_vmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Number Generation\n",
    "\n",
    "JAX handles random numbers differently from NumPy. Instead of a global random state, JAX uses explicit **keys** that you pass to random functions. This makes randomness reproducible and compatible with JIT compilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: Array((), dtype=key<fry>) overlaying:\n",
      "[ 0 42]\n"
     ]
    }
   ],
   "source": [
    "from jax import random\n",
    "\n",
    "# Create a random key (like setting a seed)\n",
    "key = random.key(42)\n",
    "print(\"Key:\", key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniform [0, 1): [0.48870957 0.6797972  0.6162715  0.5610161  0.4506446 ]\n",
      "Standard normal: [-0.02830462  0.46713185  0.29570296  0.15354592 -0.12403282]\n"
     ]
    }
   ],
   "source": [
    "uniform_samples = random.uniform(key, shape=(5,))\n",
    "print(\"Uniform [0, 1):\", uniform_samples)\n",
    "\n",
    "normal_samples = random.normal(key, shape=(5,))\n",
    "print(\"Standard normal:\", normal_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: Using the same key gives the same random numbers. To get different random numbers, you need to **split** the key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same key, call 1: [-0.02830462  0.46713185  0.29570296]\n",
      "Same key, call 2: [-0.02830462  0.46713185  0.29570296]\n",
      "\n",
      "After splitting:\n",
      "Using subkey: [ 0.60576403  0.7990441  -0.908927  ]\n"
     ]
    }
   ],
   "source": [
    "# Same key = same numbers\n",
    "print(\"Same key, call 1:\", random.normal(key, shape=(3,)))\n",
    "print(\"Same key, call 2:\", random.normal(key, shape=(3,)))\n",
    "\n",
    "# Split the key to get new keys\n",
    "key, subkey = random.split(key)\n",
    "print(\"\\nAfter splitting:\")\n",
    "print(\"Using subkey:\", random.normal(subkey, shape=(3,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 3 different samples:\n",
      "  subkey 0: [-0.21089035 -1.3627948 ]\n",
      "  subkey 1: [-1.8259704  -0.40702963]\n",
      "  subkey 2: [-1.0296261  0.3765022]\n"
     ]
    }
   ],
   "source": [
    "# You can split into multiple keys at once\n",
    "key, *subkeys = random.split(key, num=4)  # Get 3 subkeys + 1 new main key\n",
    "print(\"Generated 3 different samples:\")\n",
    "for i, sk in enumerate(subkeys):\n",
    "    print(f\"  subkey {i}: {random.normal(sk, shape=(2,))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "| Feature | What it does | Example |\n",
    "|---------|-------------|--------|\n",
    "| `jax.numpy` | NumPy-compatible array operations | `jnp.sin(x)`, `jnp.array([1,2,3])` |\n",
    "| `.at[].set()` | Immutable array updates | `x.at[0].set(10)` |\n",
    "| `jax.grad` | Automatic differentiation | `jax.grad(f)(x)` |\n",
    "| `jax.value_and_grad` | Get value and gradient together | `val, grad = jax.value_and_grad(f)(x)` |\n",
    "| `jax.jit` | Compile for speed | `jax.jit(f)(x)` |\n",
    "| `jax.vmap` | Vectorize over batches | `jax.vmap(f)(batch)` |\n",
    "| `random.key` | Create random key | `key = random.key(42)` |\n",
    "| `random.split` | Split key for new randomness | `key, subkey = random.split(key)` |\n",
    "\n",
    "These tools compose arbitrarily: `jax.jit(jax.vmap(jax.grad(f)))` is perfectly valid!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
